{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebolofis/AngularRouting/blob/master/CAM_DS_C201_Demo_2_3_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First things first** - please go to 'File' and select 'Save a copy in Drive' so that you have your own version of this activity set up and ready to use.\n",
        "Remember to update the portfolio index link to your own work once completed!"
      ],
      "metadata": {
        "id": "IDvzaLBSk2hE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 2.3.3 Manually complete a backward pass in Python\n",
        "\n",
        "Follow the demonstration to learn how to complete a backward propagation in a neural network using Python. In this demonstration, you'll learn how to:\n",
        "- initialise the weights and biases for the hidden and output layers\n",
        "- update the weights and biases\n",
        "- compute the loss derivatives\n",
        "- compute the gradients for the hidden layer parameters\n",
        "- set the learning rate."
      ],
      "metadata": {
        "id": "gEegCj7ghIf1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xKfr95ag99r"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define four functions that are necessary for computing derivatives.\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(matrix):\n",
        "    \"\"\"\n",
        "    Rectified Linear Unit (ReLU) activation function for a matrix.\n",
        "\n",
        "    Parameters:\n",
        "    matrix (numpy.ndarray): Input matrix.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Output matrix after applying the ReLU activation.\n",
        "    \"\"\"\n",
        "    return np.maximum(0, matrix)\n",
        "\n",
        "\n",
        "def relu_derivative(matrix):\n",
        "    \"\"\"\n",
        "    Compute the derivative of the Rectified Linear Unit (ReLU) activation function for a matrix.\n",
        "\n",
        "    Parameters:\n",
        "    matrix (numpy.ndarray): Input matrix.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Derivative matrix.\n",
        "    \"\"\"\n",
        "    return np.where(matrix > 0, 1, 0)\n",
        "\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    sigmoid_x = sigmoid(x)\n",
        "    return sigmoid_x * (1 - sigmoid_x)"
      ],
      "metadata": {
        "id": "BpCYQ61PhQIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise the weights and biases for the hidden layer.\n",
        "hidden_layer_weights = np.array([[0.3, 0.4], [0.5, 0.6]])\n",
        "hidden_layer_biases = np.array([0.1, 0.2])\n",
        "\n",
        "# Initialise the weights and biases for the output layer.\n",
        "output_layer_weights = np.array([[0.7], [0.8]])\n",
        "output_layer_biases = np.array([0.3])"
      ],
      "metadata": {
        "id": "sUlm3H7mhVDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the forward propagation.\n",
        "X = np.array([[0.1, 0.2]])\n",
        "\n",
        "# Hidden layer\n",
        "hidden_layer_input = np.dot(X, hidden_layer_weights) + hidden_layer_biases\n",
        "\n",
        "# Hidden_layer_output = 1 / (1 + np.exp(-hidden_layer_input)) for Sigmoid activation\n",
        "hidden_layer_output=relu(hidden_layer_input)\n",
        "\n",
        "# Output layer\n",
        "output_layer_input = np.dot(hidden_layer_output, output_layer_weights) + output_layer_biases\n",
        "final_output = 1 / (1 + np.exp(-output_layer_input))  # Sigmoid activation"
      ],
      "metadata": {
        "id": "6ImPvEWsRI1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the input and outputs.\n",
        "print(\"Input:\", X)\n",
        "print(\"Hidden layer output:\", hidden_layer_output)\n",
        "print(\"Final output:\", final_output)"
      ],
      "metadata": {
        "id": "qj8L6XzqRQs0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8282bff0-e3c5-4401-ccef-6d565f09c8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [[0.1 0.2]]\n",
            "Hidden layer output: [[0.23 0.36]]\n",
            "Final output: [[0.67896077]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the updated weights and biases.\n",
        "print(\"Original Hidden Layer Weights:\", hidden_layer_weights)\n",
        "print(\"Original Hidden Layer Biases:\", hidden_layer_biases)\n",
        "print(\"Original Output Layer Weights:\", output_layer_weights)\n",
        "print(\"Original Output Layer Biases:\", output_layer_biases)"
      ],
      "metadata": {
        "id": "yCqrS_GmRVLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b98bafa-2624-4f84-d77c-2e2c658ae31d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Hidden Layer Weights: [[0.3 0.4]\n",
            " [0.5 0.6]]\n",
            "Original Hidden Layer Biases: [0.1 0.2]\n",
            "Original Output Layer Weights: [[0.7]\n",
            " [0.8]]\n",
            "Original Output Layer Biases: [0.3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume the target for binary classification.\n",
        "target = np.array([[1]])"
      ],
      "metadata": {
        "id": "OtPbZi2zBOf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the loss derivative with respect to the final output.\n",
        "loss_derivative = final_output - target\n",
        "\n",
        "# Run backward pass through the output layer.\n",
        "output_layer_grad = loss_derivative * sigmoid_derivative(output_layer_input)"
      ],
      "metadata": {
        "id": "CnZoOB2alESe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Updating gradients for the output layer"
      ],
      "metadata": {
        "id": "JTuSbHF2-EVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the gradients for the output layer parameters.\n",
        "output_weights_grad = np.dot(hidden_layer_output.T, output_layer_grad)\n",
        "output_biases_grad = np.sum(output_layer_grad, axis=0)"
      ],
      "metadata": {
        "id": "9wMdvWYOSNMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the backward pass through the hidden layer.\n",
        "hidden_layer_grad = np.dot(output_layer_grad, output_layer_weights.T) * relu_derivative(hidden_layer_input)"
      ],
      "metadata": {
        "id": "MuCa3qq6ffbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Update gradients for the hidden layer"
      ],
      "metadata": {
        "id": "Af-ZXZ2d-Mvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the gradients for the hidden layer parameters.\n",
        "hidden_weights_grad = np.dot(X.T, hidden_layer_grad)\n",
        "hidden_biases_grad = np.sum(hidden_layer_grad, axis=0)"
      ],
      "metadata": {
        "id": "uT49AoZMfi5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Update all weights for the output and hidden layer"
      ],
      "metadata": {
        "id": "xblId7h5-R58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume a simple learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Update the weights and biases using gradient descent.\n",
        "hidden_layer_weights -= learning_rate * hidden_weights_grad\n",
        "hidden_layer_biases -= learning_rate * hidden_biases_grad\n",
        "output_layer_weights -= learning_rate * output_weights_grad\n",
        "output_layer_biases -= learning_rate * output_biases_grad"
      ],
      "metadata": {
        "id": "cAU9vqnofkAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the updated weights and biases.\n",
        "print(\"Updated Hidden Layer Weights:\", hidden_layer_weights)\n",
        "print(\"Updated Hidden Layer Biases:\", hidden_layer_biases)\n",
        "print(\"Updated Output Layer Weights:\", output_layer_weights)\n",
        "print(\"Updated Output Layer Biases:\", output_layer_biases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1LayNNlzSwH",
        "outputId": "1832bdde-05df-4172-e3da-2047407bb0f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Hidden Layer Weights: [[0.30048985 0.40055982]\n",
            " [0.50097969 0.60111965]]\n",
            "Updated Hidden Layer Biases: [0.10489845 0.20559823]\n",
            "Updated Output Layer Weights: [[0.70160949]\n",
            " [0.8025192 ]]\n",
            "Updated Output Layer Biases: [0.30699779]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key information\n",
        "This demonstration illustrated how to manually complete backward propagation in Python.\n",
        "\n",
        "## Reflect\n",
        "What are the pracitical applications of this technique?\n",
        "\n",
        "> Select the pen from the toolbar to add your entry."
      ],
      "metadata": {
        "id": "aC-9WTlI-dNk"
      }
    }
  ]
}